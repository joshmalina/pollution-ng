"use strict";angular.module("pollutionNgApp",["ngCookies","ngResource","ngRoute","angular-loading-bar","n3-line-chart","googlechart","angularMoment"]).config(["$routeProvider","$httpProvider",function(a,b){a.when("/",{templateUrl:"views/main.html",controller:"MainCtrl",controllerAs:"main"}).when("/about",{templateUrl:"views/about.html",controller:"AboutCtrl",controllerAs:"about"}).when("/contact",{templateUrl:"views/contact.html",controller:"ContactCtrl",controllerAs:"contact"}).otherwise({redirectTo:"/"})}]),angular.module("pollutionNgApp").controller("MainCtrl",["$scope","pollutionAPI","$cookies","moment","$q",function(a,b,c,d,e){function f(a){return a.map(function(b){return{x:a.indexOf(b),pollution:b.p,date:new Date(b.t_obj)}})}b.get_forecast({useLocalAPI:!1}).then(function(b){a.predictions=f(b.predictions),console.log(a.predictions),a.updatedAt=new Date(b._id),a.currently=b.currently}),a.options={axes:{x:{key:"date",type:"date",ticksRotate:-45,ticksFormat:"%a %b %d"},y:{type:"linear"}},series:[{y:"pollution",color:"#d62728",thickness:"4px"}],drawDots:!1,zoom:!0,drawLegend:!1},a.changeColor=function(a){function b(a,b,c){return a>=b&&c>=a}for(var c=[{min:0,max:50,color:"#009966"},{min:50,max:100,color:"#ffde33"},{min:100,max:150,color:"#ff9933"},{min:150,max:200,color:"#cc0033"},{min:200,max:300,color:"#660099"},{min:300,max:2e3,color:"#7e0023"}],d=c.length,e=0;d>e;e++)if(b(a,c[e].min,c[e].max))return"{'background-color': '"+c[e].color+"'}"}}]),angular.module("pollutionNgApp").controller("AboutCtrl",["$scope","pollutionAPI","$cookies",function(a,b,c){b.get_errors({useLocalAPI:!1}).then(function(b){console.log(b.raw_errors),a.raw_errors=b.raw_errors;for(var c=b.ave_errors,d=Object.keys(c),e=d.length,f=[],g=0;e>g;g++)c[d[g]].hour=g+1,f.push(c[d[g]]);a.errors=f,console.log(f);for(var h=b.ave_errors_by_val,i=Object.keys(h),j=i.length,k=[],g=0;j>g;g++)h[i[g]].index_val=parseInt(i[g]),k.push(h[i[g]]);a.errors_by_val=k,console.log(k)}),a.options={axes:{x:{key:"hour",type:"linear"},y:{type:"linear"}},series:[{y:"avg",color:"#1a0dab",thickness:"3px"}],drawDots:!1,zoom:!0,drawLegend:!0},a.options_by_val={axes:{x:{key:"index_val",type:"linear"},y:{type:"linear"}},series:[{y:"avg",color:"#1a0dab",thickness:"3px"}],drawDots:!0,zoom:!0,drawLegend:!0}}]),angular.module("pollutionNgApp").directive("linearChart",["d3Service","$q","pollutionAPI",function(a,b,c){return{template:'<svg width="700" height="200"></svg>',restrict:"EA",link:function(b,d,e){a.d3().then(function(a){c.then(function(a){a.data.predictions})})}}}]),angular.module("pollutionNgApp").factory("d3Service",["$document","$q","$rootScope",function(a,b,c){function d(){c.$apply(function(){e.resolve(window.d3)})}var e=b.defer(),f=a[0].createElement("script");f.type="text/javascript",f.async=!0,f.src="http://d3js.org/d3.v3.min.js",f.onreadystatechange=function(){"complete"==this.readyState&&d()},f.onload=d;var g=a[0].getElementsByTagName("body")[0];return g.appendChild(f),{d3:function(){return e.promise}}}]),angular.module("pollutionNgApp").factory("pollutionAPI",["$http","$cookies","$q",function(a,b,c){function d(b,c){var d=b.useLocalAPI?i:j,e={method:"GET",url:d+c};return a(e)}function e(a,c){return b.get(a)&&"undefined"!=typeof localStorage.getItem(c)}function f(a){return JSON.parse(localStorage.getItem(a))}function g(a,c){b.put(a.name,!0,{expires:(new Date).addHours(a.numHours)}),localStorage.setItem(c.name,JSON.stringify(c.data))}function h(a,b,h,i){var j=c.defer();return e(a.name,b)?j.resolve(f(b)):d(h,i).then(function(c){c=c.data,g(a,{name:b,data:c}),j.resolve(c)}),j.promise}Date.prototype.addHours=function(a){return this.setHours(this.getHours()+a),this};var i="http://127.0.0.1:5000",j="http://ec2-52-91-103-245.compute-1.amazonaws.com",k={name:"errors_available",numHours:24},l="errors",m={name:"forecast_still_valid",numHours:1},n="forecast";return{get_forecast:function(a){return h(m,n,a,"/getLatest")},get_errors:function(a){return h(k,l,a,"/getErrors")}}}]),angular.module("pollutionNgApp").controller("ContactCtrl",function(){this.awesomeThings=["HTML5 Boilerplate","AngularJS","Karma"]}),angular.module("pollutionNgApp").controller("HeaderCtrl",["$scope","$location",function(a,b){a.isActive=function(a){return a===b.path()}}]),angular.module("pollutionNgApp").run(["$templateCache",function(a){a.put("views/about.html",'<style>.vertical-text {\n  transform: rotate(270deg);\n  transform-origin: left bottom 0;\n  position: relative;\n  top:290px;\n  float:left;\n}\n\n@media only screen and (max-width: 650px) {\n	.vertical-text {\n		display:none;\n	}\n}</style> <h1 class="page-header">Behind the numbers</h1> <p class="lead">Understand the learning model used, motivation for the task, and what the data looks like</p> <p> Air pollution in China is a grave concern for the environmental and public health of the country. The air quality in the capital is much worse than what is considered healthy by international health organizations. Prolonged exposure to air pollution at the level experienced in Beijing is associated with heart disease, lung cancer and stroke. </p> <p>This project forecasts 10 days of hourly particulate estimates in Beijing based on historical pollution and weather records. Hopefully, it will allow residents to manage their exposure to very bad (or good) air a few days in advance, or just give them one more tool in the battle for clean air. When I lived in Beijing, I used <a href="http://banshirne.com">a similar tool</a> to exercise outside without wearing a <a href="http://www.greenpeace.org/eastasia/news/blog/the-three-types-of-masks-that-protect-you-fro/blog/38232/">mask</a>. <br> <h2>The data</h2> </p><p class="lead">There were roughly four kinds of data involved in this puzzle:</p> <table class="table table-condensed table-bordered"> <thead> <tr> <th>#</th> <th>Data</th> <th>Origin</th> </tr> <tr> <td>1</td><td>Historical Weather</td><td>Compiled hourly from the <a href="http://www.wunderground.com/weather/api/">Wunderground API</a></td> </tr> <tr> <td>2</td><td>Future Weather</td><td>Also available from Wunderground, as part of their 10-day forecast</td> </tr> <tr> <td>3</td><td>Historical Pollution</td><td>Collected and posted by the <a href="http://www.stateair.net/web/historical/1/1.html">US Embassy in Beijing</a></td> </tr> <tr> <td>4</td><td>Future Pollution</td><td>Our task to come up with</td> </tr> </thead> </table> <p>Each call to the Wunderground API exposed only 24 hours of weather data. With eight years of pollution data waiting to be matched, this <a href="https://github.com/joshmalina/pollution/blob/master/notebooks/Build_historical_weather_data.ipynb">required hundreds of calls to Wunderground, as well as a fair bit of cleaning, massaging and joining</a>.</p> <p>Early exploration of the data set revealed that weather factors such as wind speed, humidity and air pressure were correlated with changes in particulate density, but were also correlated with each other. Seasonal trends in pollution values (from earlier research), and the fact that the data were part of a time series pointed toward a more flexible model. Early linear approximations performed poorly out of sample. Pollution values themselves were not normally distributed, and extreme points distributed widely outside a few standard deviations of the mean were not uncommon. A candidate for such a data set is a random forest -- it is more resilient than linear models to highly correlated predictors (since it takes a random subset of predictors for the construction of each decision tree).</p> <!-- <h2>What do the numbers mean?</h2>\n\n<p>AQI is</p> --> <h2>How do machines learn?</h2> <p> The machine learning process is not so different from educating a child. It learns by exposure to diverse examples and by memorizing their characteristics as weights. In a task resembling ours, the machine might be shown a cow feeding and told, "This is a cow", and then a cow drinking water, nursing its young, or swatting flies from its buttocks -- at each instance, it is again told "This is a cow". With enough practice, and the meeting of several other conditions, the machine may learn. </p> <p> Less abstractly, the machine constructs a mathematical function of finely tuned weights -- adjusted in response to a training process that allows it to compare its would-be guess to the given answers. In this case, I\'ve used an ensemble of bootstrapped decision trees whose features were also randomly selected, known as a <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a> to guess at air pollution given weather data. It was a good fit for my problem because it was a flexible learning model that wasn\'t as resistant to high leverage points as a linear (or sort of linear) system. It also didn\'t require too much fine tuning. <img src="/images/tree_example.bc1ffaa4.png" class="img-responsive" style="margin-bottom: -40px"><label>Figure 1: A decision tree classifies vectors of features according to a series of conditions. Each input continues down the tree until arriving at a terminal node, where it waits for the rest of the data set to be processed. Upon completion, either a label or numeric value is assigned to all observations within and any new observations that happen down the same path.</label> </p><p> A decision tree sorts inputs into a set of leaves, and each leaf\'s final membership receives a label. The tree sorts by "asking" the data a series of yes-no questions: "Is humidity less than 43.5? If so, go left. Else, go right." Once all inputs are placed in a terminal node, a classification scheme or regression pattern will assign a value to that input and to all future inputs that arrive in the same node. </p> <p> A single tree is a poor model. It suffers from high variance -- small changes in inputs can lead to a very different approximation. This "instability" makes it unlikely to perform very well out of sample. Fortunately, if we grow many diverse trees and average their predictions we can lower our variance and thus our error. The sub-sampling is completed via the process <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">of bagging.</a></p> <p> The random sampling of the predictors is the "random" in random forest -- each tree is grown with fewer than all available predictors. For example, one tree may be grown on wind speed and humidity, whereas another is grown using temperature and air pressure. By preserving some variety in our forest, our final product is even better suited to correctly classifying new observations. Also, since some of our weather predictors are highly collinear, the forest spreads out the variance by not including all the predictors. In the presence of a few highly predictive variables, this randomness preserves diversity in tree construction that makes our averaging of their collective predictions meaningful, rather than the averaging of many clones.</p> <p> To get this random forest performing well, it is easy to set a large number of trees (500 or more) and allow it to build to any depth. However, since these results would need to be made available online (and there currently isn\'t a budget for a multi-node cluster of EC2 instances), it was important to minimize the size of the forest since it would be loaded into memory for the computations. Experimenting with different forest sizes showed that there was little decrease in error after about 100 trees. </p> <h2>A note on time series</h2> <p> In this time series, the pollution values are not independent -- which is to say, knowing the value of pollution at time A is helpful in knowing it at B. Every pollution value is highly correlated with its immediate neighbors. To use this to our benefit, we include in the model the prevous hour\'s pollution value as a predictor. But we must be careful to not overweight this, since when we forecast the previous hour\'s value is a mere prediction after the first hour.</p> <img src="/images/timeseries.40cbb760.png" class="img-responsive" style="width:90%"><label>Figure 2: Pollution over the course of eight years in Beijing. The average is about 100, whereas 50 is the maximum that is considered healthy by the World Health Organization.</label> <h2 id="measuringSuccess">Measuring success</h2> <p> Early estimates of error on a test-set (a fraction of the data where the model was not trained) showed that our model explained nearly 78 percent of the variety in the target output (R-squared). This is a gauge of how our target (pollution) changes when the various weather predictions changed. If there is a strong correspondence, then we would see a higher R-squared. We also had an average error of around 32 (Root mean squared error), and we should expect this to be higher as we get further from the present.</p> <p>That analysis relies on the test data set, which is not quite the same as our forecasts. Our test set\'s weather conditions are <em>actual observations</em>. Our pollution forecasts, on the other hand, rely on guesses -- the weather forecast. Also, our validation set is incomplete. Previous pollution values -- which are themselves predictors -- can only be filled in at the time of prediction -- and again, they are not recorded observations but guesses.</p> <h4>Error as a function of distance into future</h4> <p>In the figure below, we are plotting average hourly absolute error as a function of time -- 240 hours (10 days * 24 hours) worth of predictions. As we can see, the average error for the pollution value just 1 hour into the future is quite low, whereas the error jumps a fair bit after that and for the next 20 hours so, at which point it sort of flattens out. That is to say -- the model is not demonstrably worse at 5 days out then it is at 10. The one important exception: the model is very good, no matter how far in the future, if the pollution expectation is very low. </p> <div ng-if="errors" class="jumbotron"> <h2 class="text-center">Average hourly absolute error as a function of time</h2><br> <span class="vertical-text">Error as average absolue houly value </span> <linechart data="errors" options="options" mode="" width="" height="500"></linechart> <span>Number of hours in advance of the event</span><br><br> <label>Figure 3: Error as our prediction stretches into the future.</label><br> </div> <p>Here we see the evidence. Indeed, if the pollution values to be predicted are low -- that is, if it is going to be a clean day in Beijing -- it doesn\'t matter if that clean day is not for six days into the future: our model is accurate. This explains the dip in Figure 3 -- which a screen shot, rather than a dynamically driven graph.</p> <div> <h2>Snapshot: model success 6 days in advance</h2> <img src="/images/dip-at-140-hours-an.4a85bced.png"></div><br> <p> This is both good news and bad news for our model. The bad news: our model is not accurate for high pollution values. The good news: it can accurately predict a good day for Beijing\'s skies even six days before the event occurs. The bottom line: trust us on good pollution days, and when pollution is bad, we can\'t tell you how bad it is. All we can say is its not good. So, the weather is highly correlated with low pollution values, and not at all with high pollution values. Either that or the model is not doing enough with the existing data set. To more clearly witness how our model performs on various conditions that wind up producing various levels of pollution, consider figure 5.</p> <div ng-if="errors_by_val" class="jumbotron"> <h2 class="text-center">Average absolute error as a function of pollution value</h2><br> <span class="vertical-text">Absolute value of average error</span> <linechart data="errors_by_val" options="options_by_val" mode="" width="" height="500"></linechart> <span>Actual pollution values for which predictions are generated.</span><br> <br> <label>Figure 5: Our model is less accurate for higher levels of pollution. </label><br> </div> <h2>Getting the results online</h2> <p>For those interested in the engineering component of this application, the backend is running on an Amazon Elastic Compute Cloud instance, which is running ubuntu loaded with the set of scientific computing libraries provided by Anaconda. Since the random forest is kind of large and the generation of predictions a bit intensive, the memory requirements were such that the free tier of AWS was not sufficient -- though perhaps someone else could figure out how to host it for free! The application contains a Python api built using Flask, and stores predictions in a MongoDB provided MongoLab. The computations are performed with Scikit Learn\'s regression tree module, and the data was cleaned up and transformed using Pandas. The api serves up predictions to an Angular app hosted on Heroku.</p> <h2>Sources of error and improvement</h2> <p> In machine learning, error can be understood as a compositions of a fallible model plus random noise. Random noise is the variation in a data set that cannot be captured by the predictor variable. The fallability of the model itself can be understood as a composition of bias and variance, which captures a central tension in statistical learning whereby a great fit on the training data very often fails to generalize to new observations. When I first started analyzing the data, I attempted to fit a high degree polynomial function to the training data. While it achieved a similarly high R-squared value to the random forest ultimately used here, it completely failed to perform well on the test set. Of course, there may ways, given the current data set and features used, to improve the predictive power -- perhaps a different learning algorithm could fit the data a bit better. Here we will focus on those features of the data itself that probably contributed to its error.</p> <ol> <li> The pollution data and the weather data were collected approximately 12 miles away (pollution at the US Embassy, weather at the airport). If you\'ve ever seen a <a href="http://aqicn.org/city/beijing/">map</a> of pollution readings in Beijing, you know that even short distances can result in different particulate readings. I can\'t say what effect this had on the model, but it could probably be mitigated by including readings from multiple stations (which are available from the Chinese government, though not as easily accessible). </li> <li> Some potentially interesting variables are missing from the data set, such as traffic patterns and variation in factory output. I was not able to find such data, though I would be very interested in including it. </li> <li> The weather forecast (which affects the predictions, not the model) is itself fallible, and so might similarly taint our pollution forecast. </li> </ol> <h2>Sources</h2> <p class="lead">The following authors and their books and course materials either directly inspired or helped me to understand or implement these learning methods.</p> <ol> <li> Abu-Mostafa, Yaser. <em>Learning from data</em>. <a href="https://work.caltech.edu/telecourse.html"> -- An introductory machine learning course</a> </li> <li> Hastie, et al. <em>An introduction to statistical learning</em>. <a href="http://www-bcf.usc.edu/~gareth/ISL/"> Text of all things machine learning</a>, including construction of decision trees, random forests </li> <li> Hamner, Ben. <a href="http://blog.kaggle.com/2012/05/01/chucking-everything-into-a-random-forest-ben-hamner-on-winning-the-air-quality-prediction-hackathon/"> -- Write up of method for winning air pollution hackathon </a></li> <li> US Dept of State Mission in China, source of pollution data</li> <li>Wunderground API, source of weather data</li> </ol> <!-- \nimprovements:\n\n1. add some pairwise correlation images\n2. show the importance table for features of the random forest\n3. show a graph displaying error as a function of trees\n--> <!-- \nimprovements to the model:\n1. instead of using the n-5 value, train on the n-1 value, but give it less importance as n => 240\n-->'),a.put("views/contact.html","<p>If you'd like to collaborate, have questions, ways to improve the model, or anything at all: <br><br>joshuamalina [at] gmail</p>"),a.put("views/main.html",'<style>.vertical-text {\n  transform: rotate(270deg);\n  transform-origin: left bottom 0;\n  position: relative;\n  top:290px;\n  float:left;\n}\n\n@media only screen and (max-width: 650px) {\n  .vertical-text {\n    display:none;\n  }\n}</style> <div ng-if="predictions" class="jumbotron" style=""> <span class="vertical-text">Predicted AQI</span> <h2 class="text-center">Beijing pollution for the next 10 days</h2><br> <linechart data="predictions" options="options" mode="" width="" height="500"></linechart><br><br> <p>Latest: {{currently}} on {{updatedAt | date: \'EEEE, MMM d @ h a\'}}</p> <small>Beijingers: the model is much more accurate for predicting low pollution values than high ones. See the <a href="#/about">about</a> section for details.</small> </div> <h4 ng-hide="predictions" class="text-center">Rubbing crystal ball ...</h4> <table class="table table-bordered table-condensed" ng-if="predictions"> <thead> <tr> <th>Date</th> <th>Projected AQI</th> </tr> </thead> <tbody> <tr ng-repeat="obj in predictions"> <td>{{obj.date | date: \'EEEE, MMM d @ h a\'}}</td> <td>{{obj.pollution | number:0}}</td> <td ng-style="{{changeColor(obj.pollution)}}"></td> </tr> </tbody> </table>')}]);