"use strict";angular.module("pollutionNgApp",["ngCookies","ngResource","ngRoute","angular-loading-bar","n3-line-chart","googlechart","angularMoment"]).config(["$routeProvider","$httpProvider",function(a,b){a.when("/",{templateUrl:"views/main.html",controller:"MainCtrl",controllerAs:"main"}).when("/about",{templateUrl:"views/about.html",controller:"AboutCtrl",controllerAs:"about"}).when("/contact",{templateUrl:"views/contact.html",controller:"ContactCtrl",controllerAs:"contact"}).otherwise({redirectTo:"/"})}]),angular.module("pollutionNgApp").controller("MainCtrl",["$scope","pollutionAPI","$cookies","moment","$q",function(a,b,c,d,e){function f(a){return a.map(function(b){return{x:a.indexOf(b),pollution:b.p,date:new Date(b.t_obj)}})}b.get_forecast({useLocalAPI:!1}).then(function(b){a.predictions=f(b.predictions),console.log(a.predictions),a.updatedAt=new Date(b._id),a.currently=b.currently}),a.options={axes:{x:{key:"date",type:"date",ticksRotate:-45,ticksFormat:"%a %b %d"},y:{type:"linear"}},series:[{y:"pollution",color:"#d62728",thickness:"4px"}],drawDots:!1,zoom:!0,drawLegend:!1},a.changeColor=function(a){function b(a,b,c){return a>=b&&c>=a}for(var c=[{min:0,max:50,color:"#009966"},{min:50,max:100,color:"#ffde33"},{min:100,max:150,color:"#ff9933"},{min:150,max:200,color:"#cc0033"},{min:200,max:300,color:"#660099"},{min:300,max:2e3,color:"#7e0023"}],d=c.length,e=0;d>e;e++)if(b(a,c[e].min,c[e].max))return"{'background-color': '"+c[e].color+"'}"}}]),angular.module("pollutionNgApp").controller("AboutCtrl",["$scope","pollutionAPI","$cookies",function(a,b,c){b.get_errors({useLocalAPI:!1}).then(function(b){for(var c=Object.keys(b),d=c.length,e=[],f=0;d>f;f++)b[c[f]].hour=f+1,e.push(b[c[f]]);a.errors=e,console.log(e)}),a.options={axes:{x:{key:"hour",type:"linear"},y:{type:"linear"}},series:[{y:"avg",color:"#1a0dab",thickness:"3px"}],drawDots:!0,zoom:!0,drawLegend:!0}}]),angular.module("pollutionNgApp").directive("linearChart",["d3Service","$q","pollutionAPI",function(a,b,c){return{template:'<svg width="700" height="200"></svg>',restrict:"EA",link:function(b,d,e){a.d3().then(function(a){c.then(function(a){a.data.predictions})})}}}]),angular.module("pollutionNgApp").factory("d3Service",["$document","$q","$rootScope",function(a,b,c){function d(){c.$apply(function(){e.resolve(window.d3)})}var e=b.defer(),f=a[0].createElement("script");f.type="text/javascript",f.async=!0,f.src="http://d3js.org/d3.v3.min.js",f.onreadystatechange=function(){"complete"==this.readyState&&d()},f.onload=d;var g=a[0].getElementsByTagName("body")[0];return g.appendChild(f),{d3:function(){return e.promise}}}]),angular.module("pollutionNgApp").factory("pollutionAPI",["$http","$cookies","$q",function(a,b,c){function d(b,c){var d=b.useLocalAPI?i:j,e={method:"GET",url:d+c};return a(e)}function e(a,c){return b.get(a)&&"undefined"!=typeof localStorage.getItem(c)}function f(a){return JSON.parse(localStorage.getItem(a))}function g(a,c){b.put(a.name,!0,{expires:(new Date).addHours(a.numHours)}),localStorage.setItem(c.name,JSON.stringify(c.data))}function h(a,b,h,i){var j=c.defer();return e(a.name,b)?j.resolve(f(b)):d(h,i).then(function(c){c=c.data,g(a,{name:b,data:c}),j.resolve(c)}),j.promise}Date.prototype.addHours=function(a){return this.setHours(this.getHours()+a),this};var i="http://127.0.0.1:5000",j="http://ec2-52-91-103-245.compute-1.amazonaws.com",k={name:"errors_available",numHours:24},l="errors",m={name:"forecast_still_valid",numHours:1},n="forecast";return{get_forecast:function(a){return h(m,n,a,"/getLatest")},get_errors:function(a){return h(k,l,a,"/getErrors")}}}]),angular.module("pollutionNgApp").controller("ContactCtrl",function(){this.awesomeThings=["HTML5 Boilerplate","AngularJS","Karma"]}),angular.module("pollutionNgApp").controller("HeaderCtrl",["$scope","$location",function(a,b){a.isActive=function(a){return a===b.path()}}]),angular.module("pollutionNgApp").run(["$templateCache",function(a){a.put("views/about.html",'<h1 class="page-header">Behind the numbers</h1> <p class="lead">Understand the learning model used, motivation for the task, and what the data looks like</p> <p> Air pollution in China is a grave concern for the environmental and public health of the country. The air quality in the capital is on average <a href="http://www.who.int/mediacentre/factsheets/fs313/en/">twice</a> what is considered healthy by the World Health Organization. Prolonged exposure to air pollution at the level experienced in Beijing is associated with heart disease, lung cancer and stroke. </p> <p>This project forecasts 10 days of hourly particulate estimates in Beijing based on historical pollution and weather records. While it will not mitigate the problem of toxic air for the city\'s more than <a href="https://en.wikipedia.org/wiki/Beijing">20 million</a> residents, it may allow them to manage their exposure to very bad (or good) air a few days in advance. When I lived in Beijing, I used <a href="http://banshirne.com">a similar tool</a> to exercise outside without wearing a <a href="http://www.greenpeace.org/eastasia/news/blog/the-three-types-of-masks-that-protect-you-fro/blog/38232/">mask</a>. <br> <h2>The data</h2> </p><p class="lead">There were roughly four kinds of data involved in this puzzle:</p> <table class="table table-condensed table-bordered"> <thead> <tr> <th>#</th> <th>Data</th> <th>Origin</th> </tr> <tr> <td>1</td><td>Historical Weather</td><td>Compiled hourly from the <a href="http://www.wunderground.com/weather/api/">Wunderground API</a></td> </tr> <tr> <td>2</td><td>Future Weather</td><td>Also available from Wunderground, as part of their 10-day forecast</td> </tr> <tr> <td>3</td><td>Historical Pollution</td><td>Collected and posted by the <a href="http://www.stateair.net/web/historical/1/1.html">US Embassy in Beijing</a></td> </tr> <tr> <td>4</td><td>Future Pollution</td><td>Our task to come up with</td> </tr> </thead> </table> <p>Each call to the Wunderground API exposed only 24 hours of weather data. With eight years of pollution data waiting to be matched, this <a href="https://github.com/joshmalina/pollution/blob/master/notebooks/Build_historical_weather_data.ipynb">required hundreds of calls to Wunderground, as well as a fair bit of cleaning, massaging and joining</a>.</p> <p>Early exploration of the data set revealed that weather factors such as wind speed, humidity and air pressure were correlated with changes in particulate density, but were also correlated with each other. Seasonal trends in pollution values (from earlier research), and the fact that the data were part of a time series pointed toward a more flexible model. Early linear approximations performed poorly out of sample. Pollution values themselves were not normally distributed, and extreme points distributed widely outside a few standard deviations of the mean were not uncommon.</p> <h2>How do machines learn?</h2> <p> The machine learning process is not so different from educating a child. It learns by exposure to diverse examples, by memorizing their characteristics, and generalizing its experience to novel observations. In a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> task such as ours, the machine is shown a bit of data and told, "This is some thing." It could be shown a cow feeding and told, "This is a cow". Then a different cow drinking water, nursing its young, or swatting flies from its buttocks -- and at each instance, again, "This is a cow". With enough observations and an actual pattern to be learned (this is necessary), the machine may correctly label new instances. Not unlike an anxious Pinocchio suddenly turned real, or a shimmering Ariel now walking amongst the ranks of the bipedal, the machine can meet a four legged beast with a swollen pink bladder and confidently declare, "This is a cow."</p> <p> Less figuratively, that final success is the output of an algorithmically tuned function of weights (one or more for each of the relevant predictors) that enables the machine to apply a label or number to a new input. The machine starts with a naive or randomly selected set of weights, which move in one direction or another depending on how the model is built. In this case, I\'ve used an ensemble of bootstrapped decision trees whose features were also randomly selected, known as a <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a> to make predictions for the count (or density) of air pollution. It was a good fit for my problem because it was a flexible learning model that wasn\'t as resistant to high leverage points as a linear (or sort of linear) system. It also didn\'t require too much fine tuning. <img src="/images/tree_example.bc1ffaa4.png" class="img-responsive"><label>A decision tree classifies vectors of features according to a series of conditions. Each input continues down the tree until arriving at a terminal node, where it waits for the rest of the data set to be processed. Upon completion, either a label or numeric value is assigned to all observations within and any new observations that happen down the same path.</label> </p><p> A decision tree is a learning model that sorts inputs into a set of terminal nodes (or leaves), where each observation is assigned to a predictive class or value. The tree sorts by "asking" the data a series of yes-no questions. From root to leaf: "Is humidity less than 43.5? If so, go left. Else, go right." Once all inputs are placed in a terminal node, a classification scheme or regression pattern will assign a value to that input and to all future inputs that arrive in the same node. </p> <p> A single tree alone is a poor model for generalization. It suffers from high variance -- small changes in inputs can lead to a very different approximation. This "instability" makes it unlikely to perform very well out of sample. Fortunately, if we grow many diverse trees and average their predictions we can lower our variance and thus our error without biasing the model too much. The sub-sampling is completed via the process <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging.</a></p> <p> The random sampling of the predictors is the random forest part, in which each tree is grown with fewer than p predictors. For example, Tree 1 is grown on wind speed and humidity, whereas Tree 2 is grown using temperature and air pressure. By preserving some variety in our forest, our final product is even better suited to correctly classifying new observations. In the presence of a few highly predictive variables, this randomness preserves some diversity in tree construction.</p> <p> To get this random forest performing well, it is easy to set a large number of trees (500 or more) and allow it to build to any depth. However, since these results would need to be made available online (and there currently isn\'t a budget for a multi-node cluster of EC2 instances), it was important to minimize the size of the forest since it would be loaded into memory for the computations. Experimenting with different forest sizes showed that there was little decrease in error after about 100 trees. </p> <h2>A note on time series</h2> <p> The data in this project exemplify a time series, wherein the pollution values are not independently selected from a population. Every value is highly correlated with those immediately following it and immediately preceding it, allowing us to use previous values as predictors as well, but also complicating our analysis since the observations are not independent. In probability, events A and B are independent if knowing that A occurs offers no insight into the likelihood of B occurring. That is, my wearing of pajamas and the queen eating a sandwich are independent since knowing one does not help in knowing the likelihood of the other.</p> <p> Since pollution values do not tend to completely refresh in the course of hour, the particulate count at 2 pm is likely still counting those that were around at 3 p.m. Our model is greatly improved by incorporating n - k previous prediction values, but it is important not to overweight its significance -- as the model forecasts further out in time, previous pollution values are merely educated guesses, and so we need to expect a larger degree of error as we get further out. </p> <img src="/images/timeseries.40cbb760.png" class="img-responsive"><label>Pollution over the course of eight years in Beijing. The average is about 100, whereas 50 is the maximum that is considered healthy by the World Health Organization.</label> <h2>Measuring success</h2> <p> Our estimates on a test-set (a fraction of the data where the model was not trained) showed that our model explained nearly 78 percent of the variety in the target output (R-squared). This is a gauge of how our target (pollution) changes when the various weather predictions changed. If there is a strong correspondence, then we would see a higher R-squared. We also had an average error of around 32 (Root mean squared error), and we should expect this to be higher as we get further from the present.</p> <p>In the figure below, we are plotting average hourly absolute error as a function of time. Since we are making 240 predictions at each hour (10 days * 24 hours), we can evaluate how well the model performs at various points leading up to the event. As we can see, the average error for the pollution value just 1 hour into the future is quite low, whereas the error steadily increases as we get further from the present</p> <div ng-if="errors" class="jumbotron"> <h2 class="text-center">Average hourly absolute error as a function of time</h2><br> <linechart data="errors" options="options" mode="" width="" height="500"></linechart><br> <label>As our predictions stretch further into the future, our error increases.</label><br> </div> <h2>Getting the results online</h2> <p>For those interested in the engineering component of this application, the backend is running on an Amazon Elastic Compute Cloud instance, which is running ubuntu loaded with the set of scientific computing libraries provided by Anaconda. Since the random forest is kind of large and the generation of predictions a bit intensive, the memory requirements were such that the free tier of AWS was not sufficient -- though perhaps someone else could figure out how to host it for free! The application contains a Python api built using Flask, and stores predictions in a MongoDB provided MongoLab. The computations are performed with Scikit Learn\'s regression tree module, and the data was cleaned up and transformed using Pandas. The api serves up predictions to an Angular app hosted on Heroku.</p> <h2>Sources of error and improvement</h2> <p> In machine learning, error can be understood as a compositions of a fallible model plus random noise. Random noise is the variation in a data set that cannot be captured by the predictor variable. The fallability of the model itself can be understood as a composition of bias and variance, which captures a central tension in statistical learning whereby a great fit on the training data very often fails to generalize to new observations. When I first started analyzing the data, I attempted to fit a high degree polynomial function to the training data. While it achieved a similarly high R-squared value to the random forest ultimately used here, it completely failed to perform well on the test set. Of course, there may ways, given the current data set and features used, to improve the predictive power -- perhaps a different learning algorithm could fit the data a bit better. Here we will focus on those features of the data itself that probably contributed to its error.</p> <ol> <li> The pollution data and the weather data were collected approximately 12 miles away (pollution at the US Embassy, weather at the airport). If you\'ve ever seen a <a href="http://aqicn.org/city/beijing/">map</a> of pollution readings in Beijing, you know that even short distances can result in different particulate readings. I can\'t say what effect this had on the model, but it could probably be mitigated by including readings from multiple stations (which are available from the Chinese government, though not as easily accessible). </li> <li> Some potentially interesting variables are missing from the data set, such as traffic patterns and variation in factory output. I was not able to find such data, though I would be very interested in including it. </li> <li> The weather forecast (which affects the predictions, not the model) is itself fallible, and so might similarly taint our pollution forecast. </li> </ol> <h2>Sources</h2> <p class="lead">The following authors and their books and course materials either directly inspired or helped me to understand or implement these learning methods.</p> <ol> <li> Abu-Mostafa, Yaser. <em>Learning from data</em>. <a href="https://work.caltech.edu/telecourse.html"> -- An introductory machine learning course</a> </li> <li> Hastie, et al. <em>An introduction to statistical learning</em>. <a href="http://www-bcf.usc.edu/~gareth/ISL/"> Text of all things machine learning</a>, including construction of decision trees, random forests </li> <li> Hamner, Ben. <a href="http://blog.kaggle.com/2012/05/01/chucking-everything-into-a-random-forest-ben-hamner-on-winning-the-air-quality-prediction-hackathon/"> -- Write up of method for winning air pollution hackathon </a></li> <li> US Dept of State Mission in China, source of pollution data</li> <li>Wunderground API, source of weather data</li> </ol> <!-- \nimprovements:\n\n1. add some pairwise correlation images\n2. show the importance table for features of the random forest\n3. show a graph displaying error as a function of trees\n--> <!-- \nimprovements to the model:\n1. instead of using the n-5 value, train on the n-1 value, but give it less importance as n => 240\n-->'),a.put("views/contact.html","<p>If you'd like to collaborate, have questions, ways to improve the model, or anything at all: <br><br>joshuamalina [at] gmail</p>"),a.put("views/main.html",'<!--div linear-chart chart-data="predPromise"></div--><!--linechart data="predictions" options="options" mode="" width="" height=""></linechart--> <div ng-if="predictions" class="jumbotron" style=""> <h2 class="text-center">Beijing pollution for the next 10 days</h2><br> <linechart data="predictions" options="options" mode="" width="" height="500"></linechart><br><br> <p>Latest: {{currently}} on {{updatedAt | date: \'EEEE, MMM d @ h a\'}}</p> </div> <h4 ng-hide="predictions" class="text-center">Rubbing crystal ball ...</h4> <table class="table table-bordered table-condensed" ng-if="predictions"> <thead> <tr> <th>Date</th> <th>Projected AQI</th> </tr> </thead> <tbody> <tr ng-repeat="obj in predictions"> <td>{{obj.date | date: \'EEEE, MMM d @ h a\'}}</td> <td>{{obj.pollution | number:0}}</td> <td ng-style="{{changeColor(obj.pollution)}}"></td> </tr> </tbody> </table>')}]);