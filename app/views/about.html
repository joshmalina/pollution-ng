<h1 class="page-header">Behind the numbers</h1>

<p class="lead">Understand the learning model used, motivation for the task, and what the data looks like</p>

<p>
Air pollution in China is a grave concern for the enironmental and public health of the country. The air quality in the capital is on average <a href="http://www.who.int/mediacentre/factsheets/fs313/en/">twice</a> what is considered healthy by the World Health Organization. Prolonged exposure to air pollution at the level experience in Beijing is associated with heart disease, lung cancer and stroke.
</p>

<p>This project forecasts 10 days of hourly particulate estimates in Beijing based on historial pollution and weather records. It will not mitigate the problem of toxic air for the city's more than <a href="https://en.wikipedia.org/wiki/Beijing">20 million</a> residents, but it may allow them to minize their exposure on very bad days outside. When I lived in Beijing, I used <a href="http://banshirne.com">a similar tool</a> to exercise outside without wearing a <a href="http://www.greenpeace.org/eastasia/news/blog/the-three-types-of-masks-that-protect-you-fro/blog/38232/">mask</a>.
<br>
<h2>The data</h2>

<p class="lead">There were roughly four kinds of data involved in this puzzle:</p>

<table class="table table-condensed table-bordered">
	<thead>
	<tr>
		<th>#</th>
		<th>Data</th>
		<th>Origin</th>
	</tr>
	<tr>
		<td>1</td><td>Historical Weather</td><td>Compiled hourly from the <a href="http://www.wunderground.com/weather/api/">Wunderground API</a></td>
	</tr>
	<tr>
		<td>2</td><td>Future Weather</td><td>Also available from Wunderground, as part of their 10-day forecast</td>
	</tr>
	<tr>
		<td>3</td><td>Historical Pollution</td><td>Collected and posted by the <a href="http://www.stateair.net/web/historical/1/1.html">US Embassy in Beijing</a></td>
	</tr>
	<tr>	
		<td>4</td><td>Future Pollution</td><td>Our task to come up with</td>
	</tr>	
	</thead>
</table>

<p>Each call to the Wunderground API exposed only 24 hours of weather data. With eight years of pollution data available as batch csv files from the embassy, this <a href="https://github.com/joshmalina/pollution/blob/master/notebooks/Build_historical_weather_data.ipynb">required hundreds of calls to Wunderground, as well as a fair bit of cleaning, massging and joining</a>.</p>

<p>Early exploration of the data set revealed that weather factors such as wind speed, humidity and air pressure were correlated with changes in particulate density, but were also correlated with each other. Seasonal trends in pollution values (from earlier research), and the fact that the data were part of a time series ruled out a simple linear model. Pollution values themselves were not normally distributed, and extreme points distributed widely outside a few standard deviations of the mean were not uncommon.</p>

<h2>How do machines learn?</h2>

<p>
A machine is taught not unlike a child. It learns by exposure to diverse examples, by memorizing their characteristics, and generalizing its experience to novel observations. In a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> task such as ours, the machine is shown a bit of data and told, "This is some thing." It could be shown a cow feeding and told, "This is a cow". Then a different cow drinking water, nursing its young, or swatting flies from its buttocks -- and at each instance, again, "This is a cow". With enough observations and an implicit truth to be learned, the machine can correctly label new instances. Not unlike an anxious Pinnochio suddenly turned real, or a shimmering Ariel now walking amongst the ranks of the bipedal, the machine can meet a four legged spotted beast with a swollen pink bladder and an appetite for grass, and correctly decalre it a cow.</p>

<p>
That success is the output of an algorithmically tuned function of weights (one or more for each of the relevant predictors) that enables the machine to apply a label or number to a new input. Here, I've used an ensemble of bootstrapped decision trees whose features were also randomly selected, known as a <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a> to make predictions for the count (or density) of air pollution. 

<img src="/images/tree_example.png" class="img-responsive"><label>A decision tree</label>

<p>
A decision tree is a learning model that sorts out inputs into a set of terminal nodes (or leaves), where it is assigned to a predictive class or value. The tree sorts a data set by distributing its feature vectors in response to a series of questions with boolean responses. From root to leaf, it may be sorted according to questions like: "Is humidity less than 43.5? If so, go left. Else, go right." Once all inputs are placed in a terminal node, a classification scheme or regression pattern will assign a value to that input and to all future inputs that arrive in the same node. </p>

<p>
A single tree alone is a poor model for generalization. It suffers from high variance -- small changes in inputs can lead to a very different approximation. This "instability" makes it unlikely to perform very well out of sample. Fortunately, if we grow many diverse trees and average their predictions we can lower our variance and thus our error without biasing the model too much. The subsampling is completeled via the process <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging.</a></p>

<p>
The random sampling of the predictors is the random forest part, in which each tree is grown with fewer than p predictors. For example, Tree 1 is grown on windspeed and humidity, whereas Tree 2 is grown using temperature and air pressure. By preserving some variety in our forest, our final product is even better suited to correctly classifying new observations. In the presence of a few highly predictive varibles, this randomness preserves some diversity in tree construction.</p>

<h2>A note on time series</h2> 

<p>
The data in this project exemplify a time series, wherein the pollution values are not independelty selected from a population. Every value is highly correlated with those immediatelly following it and immediately preceding it, allowing us to use previous values as predictors as well, but also complicating our analysis since the observations are not independent. In probability, events A and B are indepdnent if knowing that A occurs offers no insight into the likelihood of B occuring. That is, my wearing of pajamas and the queen eating a sandwich are independent since knowing one does not help in knowing the likelihood of the other.</p>

<p>
Since pollution values do not tend to completely refresh in the course of hour, the particulate count at 2pm is likely still counting those that were around at 3 p.m. Our model is greatly improved by incorporating n - k previous prediction values, but it is important not to overweight its significance -- as the model forecasts further out in time, previous pollution values are merely educated guesses, and so we need to expect a larger degree of error as we get further out. </p>

<img src="/images/timeseries.png" class="img-responsive"><label>Pollution over the course of eight years in Beijing. The average is about 100, whereas 50 is the maximum that is considered healthy by the World Health Organization.</label>

<h2>Measuring success</h2>

<p>
So how did we do? Is the model any good, or just another <a href="http://www.catsthatlooklikehitler.com/">pointless website</a> taking up space on the internet? In our effort to make predictions, we are trying to build a model that performs well on our training data, but not so well that it merely "memorizes" it instead of "learning" from it -- this is a subtle distinction, but one worth considering. If it was only memorization that occured, it will be impossible to generalize to new observations. It would be like getting an advance copy of the final exam of some class, studying it, and performing well on an exam<sup>1</sup>. That is not learning.
</p>

<p>
Let's first consider some tree diagnostics. There are many ways to evaluate the effectiveness of your model. In our case, the proof will be in the pudding. And since the real world environment is not actually mirrored in our test data set (more on that later), we can only use these diagnostics as an approximation.</p>

<p>
For our random forest, our test R-squared was 0.78 -- not bad, considering it only goes up to 1.00. What exactly does this mean? Well the R-squared, or <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a> is a measure of how much of the data set's variability is caputured by the model. As the target variable (in this case pollution) changes, does our model change with it. Or does the data set move in ways that is not always captured by our random forest. According to Rsquared, our model did this nearly 80 percent of the time.</p>

<p>
Another measure, perhaps even more relevant, is the average error, or Root Mean Squared Error. In our case that was about 41. 41? Well, in our pollution model, we are normally off by about 41. Given that pollution ranges from about 10 to 500, and normally sits at 100 in Beijing, this is not that great. The difference between a good weather day and a moderate weather day could be those 41 points. So we should expect the model to not always be perfect.


How pure were our nodes?

Can we predict air pollution values in Beijing? Yes. Can we do it well? That question has a more detailed answer. When we build our model, we are trying to both minimize the per estimate error and explain as much of the variance in the data set as possible. These metrics are defined mathematically, and in the first case can be understood as the average distance ("in mathematical space") from our guess to the actual value. In the second case, we are trying to maximize what is called "R-squared", which is a measure of correlation between the predictor values (i.e. weather) and the predicted (pollution). In a simplistic sense, this asks how does pollution move when weather moves? If wind speed increases, but pollution just sits there unchanged, we might have a low value for "R-squared".

So how well did the model perform?

<h4>Wherefore doth our error lie?</h4>



What kinds of things are responsible for the errors? How can the model be improved? 

There are many reasons for why this model is not perfect. In machine learning applications, the error in your guess -- which can be defined in many ways, but which of often defined as your average error across all guesses, is a composition of error in your model plus something called random noise. 

The error in your model is what can theoretically be improved upon. In the case of this model, for example, that would be insufficient data sources, erroneous data, or irrelevant data masquerading as useful data.

<ul>
	Errors of precision
	<li>
	We are collecting pollution values at the US Embassy in the Chaoyang district in Beijing, but we are collecting weather data from the Airport, which is some number of KM to the north. 
	</li>
	<li>
	Since we are only collecting weather data and pollution data from two single points, we are at the mercy of whatever inacceuracies they have in <emphasis>their</emphasis> data.
	</li>	
</ul>

Weatherman error, random noise

<h4>What's next?</h4>

Currently, IBM is building a pollution forecaster that I'm sure will be amazing. They are said to be including not only weather data, but also data on traffic congestion and factory output (please email me if you know where to get this data).



I am no climate scientist. I’m not even, (cry), a data scientist. But I did live in Beijing, China for four years and I did see the sky darken and the buildings disappear, and I did wear a mask when I rode my bike, and no, I don’t smoke cigarettes. Pollution is a killer problem — really, it does. How fast will it kill you? That is also an interesting statistical question. According to some reports (need attribution), every year in a city like Beijing leads to a 5-6 day loss in life. Of course, this is not an informative statistic, which leads to our first lesson: the paucity of means. 

A mean (or average, as the lay people call it) is, roughly:

the sum total of some things / the number of those things.

So, in our case, this would be:

(total time lost to pollution causing death in a population / number of people in the population)

What’s misleading about this statistic is that nobody dies five days early from pollution. That wouldn’t be so bad. When you die from pollution, you die from heart disease, stroke, lung cancer, emphysema, or some other wonderful ailment. And when you die from those, you die 10 years early, or 20 years early. But most people don’t die of those ailments, and so the mean gives you 5 days. 

A better statistic would be the percent increase in those ailments mentioned (heart disease, etc…) from living in a polluted environment. Then you could say: “Alright, living here makes me 20 percent more likely to have a stroke” (get correct number).

So, we’ve established the why — well, sort of. Everyone would agree that health, and therefore pollution, might be an interesting thing to predict — but it’s no so simple. As a matter of fact, predicting pollution values would probably be of great interest to Chinese and foreigners living in Beijing — if you could predict it years into the future. That way, they could decide if they want to stick around, raise a family — they’d be more likely to if they knew the air quality would get better. But that is not at all what I’m doing here. I’m only predicting 10 days in advance — and given the normal human gestation period of 9 months — I’m not sure how relevant the predictions will be.

Well, I can tell you when it would be useful, since I used to use something similar <link_to_smog_cast>, when I lived in Beijing, to schedule my exercise. That app would tell me when the next time it was going to be decent outside, and so I would plan my jogs around it — it was very nice. Now I’ve tried to do the same and a bit more, giving numeric values on the hour, every hour, for ten days in the future.

Again, it is not so easily done. But it gave me an opportunity to explore a problem that used to be quite important to me (I don’t live in Beijing anymore) and allowed me to dive into statistics and machine learning and pro-human hacking, which is a neat thing.

<h2>Sources</h2>

<ol>
	<li>
Abu-Mostafa, Yaser. Learning from data. https://work.caltech.edu/telecourse.html -- An introductory machine learning course
	</li>
	<li>
Hastie, et al. An introduction to statistical learning. http://www-bcf.usc.edu/~gareth/ISL/ -- Great write up of all things machine learning, including construction of decision trees, random forests
	</li>
	<li>
	Hamner, Ben. http://blog.kaggle.com/2012/05/01/chucking-everything-into-a-random-forest-ben-hamner-on-winning-the-air-quality-prediction-hackathon/ -- Write up of method for winning air pollution hackathon </li>
	<li>
	US Dept of State Mission in China</li>
	<li>Wunderground API</li>
</ol>


1 For selfish reasons, though, I really just wanted to play with some machine learning algorithms. 

Appendix:

What is a decision tree?

In the specific learning model adapted here, that can be considered from a few different perspectives -- in the world of random foress -- those are called node purity and "importance". Node purity is pretty straightforward. 

The decision trees that make up our random forest arrive at their conclusions by asking the data a series of questions, and then moving the point down one branch or another based on its answer. Not unlike the parlor game 50 questions, it might ask: is your wind speed greater than 10 mph? If so, go left -- else, go right. It continues to ask this question until it can go no further -- it reaches a "terminal node" or leaf. When all the data has been hung on the leaves of the tree, each leaf is inspected to see how alike are those data points that wound up there. If they are all very alike, we consider the node to be pure, then scratch the model under the chin and say, good job.

The other measure, "importance", is also quite intuitive. It is a measure of how much worse the model would have performed had we taken out a certain predictor.

