<h1 class="page-header">Behind the numbers</h1>

<p class="lead">Understand the learning model used, motivation for the task, and what the data looks like</p>

<h2>Why forecast pollution?</h2>

<p>
Air pollution in China is serious business. The average particulate count in Beijing is about <a href="http://www.who.int/mediacentre/factsheets/fs313/en/">double</a> what the World Health Organization considers healthy. Prolonged exposure is correlated with heart and lung disease and stroke. The air is often so polluted that the blue of the sky is literally invisible, buildings are hidden, and on very bad days, outdoor school activities are cancelled and people are encouraged to stay inside.
</p>

<p>So how will forecasting pollution ten days in advance help? Mostly, it won't. For me, the impetus to start the project was to solve a personal problem. When I lived in Beijing, I would only run outside if the pollution index was below 100, and I would run without a mask on if it was below 50. But I couldn't plan around it. If there was someway to know in advance,  I wouldn't even wear a mask. But I couldn't plan to exercise -- just respond when the air cleared up. As it turns out, <a href="http://banshirne.com"/>someone</a> had already started working on a solution. I've decided to take it a step further, and learn some neat machine learning tools along the way.</p>

<h2>Modelling a time series</h2> 

<p>
Unlike many introductory problems in statistics, the forecasting of pollution values  is complicated by time. In time series data, a fundamental assumption of the distribution of observations is violated: they are not independent.
</p>

<p>
In probability, two events A and B are indepdnent if knowing that A occurs offers no insight into the likelihood of B occuring. That is, my wearing of pajamas and the queen eating a sandwich are independent events because knowing one does not help in knowing the other. Of course, this example is absurd. As we all know, the queen never eats sandwiches. Still, it demonstrates the idea of independence for which the observations in a time series do not abide.</p>

<p>
In our case, knowing the particulate count at 2 p.m. bears on our knowing the number of particles at 3 p.m. Absent a terribly foreful wind, many of those particles will still be in the air an hour later -- there is a correlation across our inputs. Therefore, in order to accurately predict what happens at time t = 0, we can't just rely on external predictors (the weather, traffic patterns) -- we must also consider what the pollution count was at t = -1, t = - 2, and so on.</p>

<h2>Sources of data</h2>

<p>There are roughly four kinds of data involved in this puzzle, three of which will be given to us. They are:</p>

<table class="table table-condensed table-bordered">
	<thead>
	<tr>
		<td>1</td><td>Historical Weather Data</td><td>Compiled from the Wunderground API</td>
	</tr>
	<tr>
		<td>2</td><td>Future Weather Data</td><td>Also available from Wunderground, as part of their 10-day forecast</td>
	</tr>
	<tr>
		<td>3</td><td>Historical Pollution Data</td><td>Collected and posted by the US Embassy in Beijing</td>
	</tr>
	<tr>	
		<td>4</td><td>Future Pollution Data</td><td>Our task to come up with</td>
	</tr>	
	</thead>
</table>

<p>The important features of the weather data included wind speed, wind direction, humidity, air pressure, and temperature. The historical data is provided hourly by Wunderground, but for each call of the API, only one day of values is returned. Thus, it required a fair bit of scripting to <a href="https://github.com/joshmalina/pollution/blob/master/notebooks/Build_historical_weather_data.ipynb">cull, clean and put together</a>. Their 10 day forecast is a bit easier to work with. </p>

<p>
The historical pollution data, more easily accessed, can be found on the <a href="http://www.stateair.net/web/historical/1/1.html">website</a> of the US Embassy in Beijing -- which maintains an hourly reading of pollution levels going back eight years -- and you can download an entire year's worth of data at time</p>

<p>The last piece of the puzzle -- future pollution -- was of course, the task at hand. More on that in the next section</p>

<h2>Learning from data</h2>

<p>
After joining the historical weather data with their respective historical pollution values, it was time to do teach the machine to learn. This process is not unlike teaching a human child.</p>

<p>
The machine learns by seeing many examples of the same thing, memorizing its characteristics, and then generalizing its experience to novel examples in the real world. In the simplest case of machine learning, known as <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a>it is shown a thing and then told, "This is that thing." Shown a cow eating, it is told, "This is a cow". Shown a cow drinking water, nursing its young, or swatting flies from its buttocks -- and at each instance, it is again told, "This is a cow". With enough diverse examples and a bit of luck, the machine can realize the dreams of Pinnochio and Ariel: to be where the people are, out in the real world. Then, when it comes upon a four legged, black and white spotted beast, with a fat pink bladder suspended from its belly, it can approach it confidently and declare, "This is a cow."</p>

<p>
Metaphors aside, we need our machine to create a mathematical function that is finely tuned to the labeling of pollution data with numerical values. How is this accomplished? For this task, I chose to use an ensemble of randomly grown decision trees, known in the machine learning literature as a <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a>. The decision tree is quite intuitive. An example of one is shown below:</p>  

<img src="/images/tree_example.png" class="img-responsive">

<p>
Basically, moving from top to bottom, the tree asks our row of data a series of questions. In this tree, it first asks: "Is your humidity less than 43.5? If so, go left. Else, go right." It continues to interrogate the row of data until it reaches a "terminal node", or leaf, where it must await the rest of the data set to move through the tree. Upon conclusion, the algorithm analyzes the constituents of each leaf, and assigns a numerical value (in our case) to all future values that might arrive there. </p>

<p>
One tree alone is not sufficient for correctly labeling unseen (future) data points. The single tree is highly "variable", which is to say that if we switch the points used to build the tree and grow it again, it would come out very differently the second time around. This "instability" makes it unlikely to perform very well out of sample, as it were, and so we must grow very many trees and average their decisions together. For those interested, this can be accomplished through a process known as <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging</a></p>

<p>
One additional layer of complexity is the "random" part of our random forest. This means that all the trees in our forest are not created equal. In random forest construction, each three is grown with only a randomly selected subset of the available predictors (that is, maybe Tree 1 is grown using the variables windspeed and humidity, whereas Tree 2 is grown using temperature and air pressure). By preserving some variety in our forest, our final product is even better suited to correctly classifying new observations.</p>

<h2>Measuring success</h2>

<p>
So how did we do? Is the model any good, or just another <a href="http://www.catsthatlooklikehitler.com/">pointless website</a> taking up space on the internet? In our effort to make predictions, we are trying to build a model that performs well on our training data, but not so well that it merely "memorizes" it instead of "learning" from it -- this is a subtle distinction, but one worth considering. If it was only memorization that occured, it will be impossible to generalize to new observations. It would be like getting an advance copy of the final exam of some class, studying it, and performing well on an exam<sup>1</sup>. That is not learning.
</p>

<p>
Let's first consider some tree diagnostics. There are many ways to evaluate the effectiveness of your model. In our case, the proof will be in the pudding. And since the real world environment is not actually mirrored in our test data set (more on that later), we can only use these diagnostics as an approximation.</p>

<p>
For our random forest, our test R-squared was 0.78 -- not bad, considering it only goes up to 1.00. What exactly does this mean? Well the R-squared, or <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a> is a measure of how much of the data set's variability is caputured by the model. As the target variable (in this case pollution) changes, does our model change with it. Or does the data set move in ways that is not always captured by our random forest. According to Rsquared, our model did this nearly 80 percent of the time.</p>

<p>
Another measure, perhaps even more relevant, is the average error, or Root Mean Squared Error. In our case that was about 41. 41? Well, in our pollution model, we are normally off by about 41. Given that pollution ranges from about 10 to 500, and normally sits at 100 in Beijing, this is not that great. The difference between a good weather day and a moderate weather day could be those 41 points. So we should expect the model to not always be perfect.


How pure were our nodes?

Can we predict air pollution values in Beijing? Yes. Can we do it well? That question has a more detailed answer. When we build our model, we are trying to both minimize the per estimate error and explain as much of the variance in the data set as possible. These metrics are defined mathematically, and in the first case can be understood as the average distance ("in mathematical space") from our guess to the actual value. In the second case, we are trying to maximize what is called "R-squared", which is a measure of correlation between the predictor values (i.e. weather) and the predicted (pollution). In a simplistic sense, this asks how does pollution move when weather moves? If wind speed increases, but pollution just sits there unchanged, we might have a low value for "R-squared".

So how well did the model perform?

<h4>Wherefore doth our error lie?</h4>



What kinds of things are responsible for the errors? How can the model be improved? 

There are many reasons for why this model is not perfect. In machine learning applications, the error in your guess -- which can be defined in many ways, but which of often defined as your average error across all guesses, is a composition of error in your model plus something called random noise. 

The error in your model is what can theoretically be improved upon. In the case of this model, for example, that would be insufficient data sources, erroneous data, or irrelevant data masquerading as useful data.

<ul>
	Errors of precision
	<li>
	We are collecting pollution values at the US Embassy in the Chaoyang district in Beijing, but we are collecting weather data from the Airport, which is some number of KM to the north. 
	</li>
	<li>
	Since we are only collecting weather data and pollution data from two single points, we are at the mercy of whatever inacceuracies they have in <emphasis>their</emphasis> data.
	</li>	
</ul>

Weatherman error, random noise

<h4>What's next?</h4>

Currently, IBM is building a pollution forecaster that I'm sure will be amazing. They are said to be including not only weather data, but also data on traffic congestion and factory output (please email me if you know where to get this data).



I am no climate scientist. I’m not even, (cry), a data scientist. But I did live in Beijing, China for four years and I did see the sky darken and the buildings disappear, and I did wear a mask when I rode my bike, and no, I don’t smoke cigarettes. Pollution is a killer problem — really, it does. How fast will it kill you? That is also an interesting statistical question. According to some reports (need attribution), every year in a city like Beijing leads to a 5-6 day loss in life. Of course, this is not an informative statistic, which leads to our first lesson: the paucity of means. 

A mean (or average, as the lay people call it) is, roughly:

the sum total of some things / the number of those things.

So, in our case, this would be:

(total time lost to pollution causing death in a population / number of people in the population)

What’s misleading about this statistic is that nobody dies five days early from pollution. That wouldn’t be so bad. When you die from pollution, you die from heart disease, stroke, lung cancer, emphysema, or some other wonderful ailment. And when you die from those, you die 10 years early, or 20 years early. But most people don’t die of those ailments, and so the mean gives you 5 days. 

A better statistic would be the percent increase in those ailments mentioned (heart disease, etc…) from living in a polluted environment. Then you could say: “Alright, living here makes me 20 percent more likely to have a stroke” (get correct number).

So, we’ve established the why — well, sort of. Everyone would agree that health, and therefore pollution, might be an interesting thing to predict — but it’s no so simple. As a matter of fact, predicting pollution values would probably be of great interest to Chinese and foreigners living in Beijing — if you could predict it years into the future. That way, they could decide if they want to stick around, raise a family — they’d be more likely to if they knew the air quality would get better. But that is not at all what I’m doing here. I’m only predicting 10 days in advance — and given the normal human gestation period of 9 months — I’m not sure how relevant the predictions will be.

Well, I can tell you when it would be useful, since I used to use something similar <link_to_smog_cast>, when I lived in Beijing, to schedule my exercise. That app would tell me when the next time it was going to be decent outside, and so I would plan my jogs around it — it was very nice. Now I’ve tried to do the same and a bit more, giving numeric values on the hour, every hour, for ten days in the future.

Again, it is not so easily done. But it gave me an opportunity to explore a problem that used to be quite important to me (I don’t live in Beijing anymore) and allowed me to dive into statistics and machine learning and pro-human hacking, which is a neat thing.

<h2>Sources</h2>

<ol>
	<li>
Abu-Mostafa, Yaser. Learning from data. https://work.caltech.edu/telecourse.html -- An introductory machine learning course
	</li>
	<li>
Hastie, et al. An introduction to statistical learning. http://www-bcf.usc.edu/~gareth/ISL/ -- Great write up of all things machine learning, including construction of decision trees, random forests
	</li>
	<li>
	Hamner, Ben. http://blog.kaggle.com/2012/05/01/chucking-everything-into-a-random-forest-ben-hamner-on-winning-the-air-quality-prediction-hackathon/ -- Write up of method for winning air pollution hackathon </li>
	<li>
	US Dept of State Mission in China</li>
	<li>Wunderground API</li>
</ol>


Appendix:

What is a decision tree?

In the specific learning model adapted here, that can be considered from a few different perspectives -- in the world of random foress -- those are called node purity and "importance". Node purity is pretty straightforward. 

The decision trees that make up our random forest arrive at their conclusions by asking the data a series of questions, and then moving the point down one branch or another based on its answer. Not unlike the parlor game 50 questions, it might ask: is your wind speed greater than 10 mph? If so, go left -- else, go right. It continues to ask this question until it can go no further -- it reaches a "terminal node" or leaf. When all the data has been hung on the leaves of the tree, each leaf is inspected to see how alike are those data points that wound up there. If they are all very alike, we consider the node to be pure, then scratch the model under the chin and say, good job.

The other measure, "importance", is also quite intuitive. It is a measure of how much worse the model would have performed had we taken out a certain predictor.

