<h1 class="page-header">Behind the numbers</h1>

<p class="lead">Understand the learning model used, motivation for the task, and what the data looks like</p>

<p>
Air pollution in China is a serious problem affecting the environment, public health, and even political stability. The average air quality in the capital Beijing is roughly <a href="http://www.who.int/mediacentre/factsheets/fs313/en/">twice</a> the maximum considered healthy by the World Health Organization. Prolonged exposure is associated with heart disease, lung cancer and stroke.
</p>

<p>This project attempts to forecast 10 days of houlry pollution estimates in Beijing. While it does nothing to mitigate the problem of toxic air for the city's more than <a href="https://en.wikipedia.org/wiki/Beijing">20 million</a> residents, it may increase awareness, and more importantly, may permit them to schedule their outside time to minimize exposure. When I lived in Beijing, I used <a href="http://banshirne.com">a similar tool</a> to exercise outside without wearing a <a href="http://www.greenpeace.org/eastasia/news/blog/the-three-types-of-masks-that-protect-you-fro/blog/38232/">mask</a>.
<br>
<h2>The data</h2>

<p class="lead">There were roughly four kinds of data involved in this puzzle:</p>

<table class="table table-condensed table-bordered">
	<thead>
	<tr>
		<th>#</th>
		<th>Data</th>
		<th>Origin</th>
	</tr>
	<tr>
		<td>1</td><td>Historical Weather</td><td>Compiled hourly from the <a href="http://www.wunderground.com/weather/api/">Wunderground API</a></td>
	</tr>
	<tr>
		<td>2</td><td>Future Weather</td><td>Also available from Wunderground, as part of their 10-day forecast</td>
	</tr>
	<tr>
		<td>3</td><td>Historical Pollution</td><td>Collected and posted by the <a href="http://www.stateair.net/web/historical/1/1.html">US Embassy in Beijing</a></td>
	</tr>
	<tr>	
		<td>4</td><td>Future Pollution</td><td>Our task to come up with</td>
	</tr>	
	</thead>
</table>

<p>Each call to the Wunderground API exposed only 24 hours of weather data. With eight years of pollution data available as batch csv files from the embassy, this <a href="https://github.com/joshmalina/pollution/blob/master/notebooks/Build_historical_weather_data.ipynb">required hundreds of calls to Wunderground, as well as a fair bit of cleaning, massging and joining</a>.</p>

<p>A bit of early discovery revealed that correlated features might be wind speed, humidity and air pressure</p>


The important features of the weather data included wind speed, wind direction, humidity, air pressure, and temperature. These features needed to be joined to the corresponding (in time) pollution values, and one massive data set created from their merger.
<p>

IMG OF SOME PAIRWISE CORRELATION

<h2>Learning from data</h2>

<p>
A machine learning process is not unlike teaching a child. It learns by seeing many examples of the same thing, memorizing its characteristics, and then generalizing its experience to novel examples. In our <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> task, the machine is shown a thing and told, "This is some thing." For example, the computer might be shown a cow feeding and told, "This is a cow". Then it is shown a cow drinking water, nursing its young, or swatting flies from its buttocks -- and at each instance, it is again told, "This is a cow". With a large enough sample of diverse examples (so that it reflects the variety of cows in the population), the machine can correctly label unlabeled observations. Not unlike a wooden Pinnochio suddenly turned real, or a shimmering Ariel now joining the ranks of the bipedal, the machine can confidently saunter up to a four legged spotted beast, with a swollen pink bladder suspended from its belly, and confidently declare, "This is a cow."</p>

<p>
In actuality, the machine arrives at a mathematical function of finely tuned weights that enable it to apply a class label or numeric value to a new vector of features. After decisiding that the relationship between features and their target was not linear, I decided to grow an ensemble of bootstrapped decision trees whose features were also randomly selected, known as a <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a>. The general structure of a decision tree:</p>  

<img src="/images/tree_example.png" class="img-responsive"><label>A decision tree</label>

<p>
Basically, moving from top to bottom, the tree asks our row of data a series of questions. In this tree, it first asks: "Is your humidity less than 43.5? If so, go left. Else, go right." It continues to interrogate the row of data until it reaches a "terminal node", or leaf, where it must await the rest of the data set to move through the tree. Upon conclusion, the algorithm analyzes the constituents of each leaf, and assigns a numerical value (in our case) to all future values that might arrive there. </p>

<p>
One tree alone is not sufficient for correctly labeling unseen (future) data points. The single tree is highly "variable", which is to say that if we switch the points used to build the tree and grow it again, it would come out very differently the second time around. This "instability" makes it unlikely to perform very well out of sample, as it were, and so we must grow very many trees and average their decisions together. For those interested, this can be accomplished through a process known as <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging</a></p>

<p>
One additional layer of complexity is the "random" part of our random forest. This means that all the trees in our forest are not created equal. In random forest construction, each three is grown with only a randomly selected subset of the available predictors (that is, maybe Tree 1 is grown using the variables windspeed and humidity, whereas Tree 2 is grown using temperature and air pressure). By preserving some variety in our forest, our final product is even better suited to correctly classifying new observations.</p>

<h2>Modelling a time series</h2> 

<p>
As a time series,  the time  The hourly observation of pollution values the forecasting of pollution values  is complicated by time. In time series data, a fundamental assumption of the distribution of observations is violated: they are not independent.
</p>

<p>
In probability, two events A and B are indepdnent if knowing that A occurs offers no insight into the likelihood of B occuring. That is, my wearing of pajamas and the queen eating a sandwich are independent events because knowing one does not help in knowing the other. Of course, this example is absurd. As we all know, the queen never eats sandwiches. Still, it demonstrates the idea of independence for which the observations in a time series do not abide.</p>

<p>
In our case, knowing the particulate count at 2 p.m. bears on our knowing the number of particles at 3 p.m. Absent a terribly foreful wind, many of those particles will still be in the air an hour later -- there is a correlation across our inputs. Therefore, in order to accurately predict what happens at time t = 0, we can't just rely on external predictors (the weather, traffic patterns) -- we must also consider what the pollution count was at t = -1, t = - 2, and so on.</p>

<img src="/images/timeseries.png" class="img-responsive"><label>Pollution over the course of eight years in Beijing. The average is about 100, whereas 50 is the maximum that is considered healthy by the World Health Organization.</label>

<h2>Measuring success</h2>

<p>
So how did we do? Is the model any good, or just another <a href="http://www.catsthatlooklikehitler.com/">pointless website</a> taking up space on the internet? In our effort to make predictions, we are trying to build a model that performs well on our training data, but not so well that it merely "memorizes" it instead of "learning" from it -- this is a subtle distinction, but one worth considering. If it was only memorization that occured, it will be impossible to generalize to new observations. It would be like getting an advance copy of the final exam of some class, studying it, and performing well on an exam<sup>1</sup>. That is not learning.
</p>

<p>
Let's first consider some tree diagnostics. There are many ways to evaluate the effectiveness of your model. In our case, the proof will be in the pudding. And since the real world environment is not actually mirrored in our test data set (more on that later), we can only use these diagnostics as an approximation.</p>

<p>
For our random forest, our test R-squared was 0.78 -- not bad, considering it only goes up to 1.00. What exactly does this mean? Well the R-squared, or <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a> is a measure of how much of the data set's variability is caputured by the model. As the target variable (in this case pollution) changes, does our model change with it. Or does the data set move in ways that is not always captured by our random forest. According to Rsquared, our model did this nearly 80 percent of the time.</p>

<p>
Another measure, perhaps even more relevant, is the average error, or Root Mean Squared Error. In our case that was about 41. 41? Well, in our pollution model, we are normally off by about 41. Given that pollution ranges from about 10 to 500, and normally sits at 100 in Beijing, this is not that great. The difference between a good weather day and a moderate weather day could be those 41 points. So we should expect the model to not always be perfect.


How pure were our nodes?

Can we predict air pollution values in Beijing? Yes. Can we do it well? That question has a more detailed answer. When we build our model, we are trying to both minimize the per estimate error and explain as much of the variance in the data set as possible. These metrics are defined mathematically, and in the first case can be understood as the average distance ("in mathematical space") from our guess to the actual value. In the second case, we are trying to maximize what is called "R-squared", which is a measure of correlation between the predictor values (i.e. weather) and the predicted (pollution). In a simplistic sense, this asks how does pollution move when weather moves? If wind speed increases, but pollution just sits there unchanged, we might have a low value for "R-squared".

So how well did the model perform?

<h4>Wherefore doth our error lie?</h4>



What kinds of things are responsible for the errors? How can the model be improved? 

There are many reasons for why this model is not perfect. In machine learning applications, the error in your guess -- which can be defined in many ways, but which of often defined as your average error across all guesses, is a composition of error in your model plus something called random noise. 

The error in your model is what can theoretically be improved upon. In the case of this model, for example, that would be insufficient data sources, erroneous data, or irrelevant data masquerading as useful data.

<ul>
	Errors of precision
	<li>
	We are collecting pollution values at the US Embassy in the Chaoyang district in Beijing, but we are collecting weather data from the Airport, which is some number of KM to the north. 
	</li>
	<li>
	Since we are only collecting weather data and pollution data from two single points, we are at the mercy of whatever inacceuracies they have in <emphasis>their</emphasis> data.
	</li>	
</ul>

Weatherman error, random noise

<h4>What's next?</h4>

Currently, IBM is building a pollution forecaster that I'm sure will be amazing. They are said to be including not only weather data, but also data on traffic congestion and factory output (please email me if you know where to get this data).



I am no climate scientist. I’m not even, (cry), a data scientist. But I did live in Beijing, China for four years and I did see the sky darken and the buildings disappear, and I did wear a mask when I rode my bike, and no, I don’t smoke cigarettes. Pollution is a killer problem — really, it does. How fast will it kill you? That is also an interesting statistical question. According to some reports (need attribution), every year in a city like Beijing leads to a 5-6 day loss in life. Of course, this is not an informative statistic, which leads to our first lesson: the paucity of means. 

A mean (or average, as the lay people call it) is, roughly:

the sum total of some things / the number of those things.

So, in our case, this would be:

(total time lost to pollution causing death in a population / number of people in the population)

What’s misleading about this statistic is that nobody dies five days early from pollution. That wouldn’t be so bad. When you die from pollution, you die from heart disease, stroke, lung cancer, emphysema, or some other wonderful ailment. And when you die from those, you die 10 years early, or 20 years early. But most people don’t die of those ailments, and so the mean gives you 5 days. 

A better statistic would be the percent increase in those ailments mentioned (heart disease, etc…) from living in a polluted environment. Then you could say: “Alright, living here makes me 20 percent more likely to have a stroke” (get correct number).

So, we’ve established the why — well, sort of. Everyone would agree that health, and therefore pollution, might be an interesting thing to predict — but it’s no so simple. As a matter of fact, predicting pollution values would probably be of great interest to Chinese and foreigners living in Beijing — if you could predict it years into the future. That way, they could decide if they want to stick around, raise a family — they’d be more likely to if they knew the air quality would get better. But that is not at all what I’m doing here. I’m only predicting 10 days in advance — and given the normal human gestation period of 9 months — I’m not sure how relevant the predictions will be.

Well, I can tell you when it would be useful, since I used to use something similar <link_to_smog_cast>, when I lived in Beijing, to schedule my exercise. That app would tell me when the next time it was going to be decent outside, and so I would plan my jogs around it — it was very nice. Now I’ve tried to do the same and a bit more, giving numeric values on the hour, every hour, for ten days in the future.

Again, it is not so easily done. But it gave me an opportunity to explore a problem that used to be quite important to me (I don’t live in Beijing anymore) and allowed me to dive into statistics and machine learning and pro-human hacking, which is a neat thing.

<h2>Sources</h2>

<ol>
	<li>
Abu-Mostafa, Yaser. Learning from data. https://work.caltech.edu/telecourse.html -- An introductory machine learning course
	</li>
	<li>
Hastie, et al. An introduction to statistical learning. http://www-bcf.usc.edu/~gareth/ISL/ -- Great write up of all things machine learning, including construction of decision trees, random forests
	</li>
	<li>
	Hamner, Ben. http://blog.kaggle.com/2012/05/01/chucking-everything-into-a-random-forest-ben-hamner-on-winning-the-air-quality-prediction-hackathon/ -- Write up of method for winning air pollution hackathon </li>
	<li>
	US Dept of State Mission in China</li>
	<li>Wunderground API</li>
</ol>


1 For selfish reasons, though, I really just wanted to play with some machine learning algorithms. 

Appendix:

What is a decision tree?

In the specific learning model adapted here, that can be considered from a few different perspectives -- in the world of random foress -- those are called node purity and "importance". Node purity is pretty straightforward. 

The decision trees that make up our random forest arrive at their conclusions by asking the data a series of questions, and then moving the point down one branch or another based on its answer. Not unlike the parlor game 50 questions, it might ask: is your wind speed greater than 10 mph? If so, go left -- else, go right. It continues to ask this question until it can go no further -- it reaches a "terminal node" or leaf. When all the data has been hung on the leaves of the tree, each leaf is inspected to see how alike are those data points that wound up there. If they are all very alike, we consider the node to be pure, then scratch the model under the chin and say, good job.

The other measure, "importance", is also quite intuitive. It is a measure of how much worse the model would have performed had we taken out a certain predictor.

